# SAM2 Analysis Pipeline
This project provides a framework for evaluating computer vision models, focusing on the **Segment Anything Model 2 (SAM2)**, released by Meta AI (FAIR) as a successor to the original SAM. SAM2 advances the state-of-the-art by unifying promptable segmentation capabilities for both **images and videos** within a single model.

Architecturally, SAM2 builds upon its predecessor, employing powerful image encoders (like the **Hiera** architecture used in some variants) and a promptable mask decoder. The key innovation enabling efficient video processing is the introduction of a **memory bank**. This module allows the model to maintain and propagate context (such as object identities and locations) across consecutive video frames, enabling **real-time, consistent segmentation** without recomputing from scratch for every frame. This makes SAM2 significantly more efficient than previous models, particularly for video-based tasks.

This project/repo aims to facilitate **robustness testing** of SAM2. It focuses on systematically evaluating model performance on custom datasets, particularly under challenging conditions or image degradations, using configurable evaluation pipelines and metrics.

## Features

* Configurable Evaluation: Run evaluations defined by simple JSON configuration files.
* SAM2 Automatic Mask Generation: Evaluates masks generated by `SamAutomaticMaskGenerator` against ground truth masks.
* Hierarchical Data Support: Handles datasets where multiple versions (e.g., degraded images) exist for each base image.
* Metric Calculation: Calculates Intersection over Union (IoU) and Boundary F1 Score (BF1) to compare predicted masks against ground truth.
* Extensible: Designed to potentially incorporate other models or evaluation pipelines in the future.

## Evaluation Dataset

The initial evaluation dataset used with this pipeline consists of images randomly sampled from the popular [COCO (Common Objects in Context)](https://cocodataset.org/) dataset. A key filtering criterion was applied: only images containing **a single ground truth object mask** were selected.

This simplification was made to streamline the mask matching and comparison process between the model's generated masks (e.g., from SAM2's `SamAutomaticMaskGenerator`) and the single ground truth object. Future work might involve datasets with multiple objects per image.

The specific images and their corresponding ground truth masks (encoded in COCO RLE format) are referenced within the input JSON file provided to the evaluation pipeline (see `config/sam2_eval_config.json` for an example structure). This file acts as the central map linking image identifiers/paths to the necessary ground truth data for evaluation.

## Prerequisites

Before running the pipeline, ensure you have the following prerequisites installed and set up:

1. **Python:** Python 3.8 or higher is recommended. You can check your version with `python --version`.
2. **Git:** Required for cloning the necessary repositories. Check with `git --version`.
3. **Virtual Environment (Strongly Recommended):** Using a virtual environment is crucial to avoid dependency conflicts. Follow these steps **before** installing other dependencies:

   * **Create the environment:** Navigate to the project root directory (`SAM2_analysis`) in your terminal and run:

     ```bash
     python -m venv venv
     ```

     (This creates a `venv` directory within your project).

   * **Activate the environment:**
     * **macOS/Linux (bash/zsh):**

       ```bash
       source venv/bin/activate
       ```

     * **Windows (Command Prompt):**

       ```cmd
       venv\Scripts\activate.bat
       ```

     * **Windows (PowerShell):**

       ```powershell
       venv\Scripts\Activate.ps1
       ```

     (Your terminal prompt should change, often showing `(venv)` at the beginning, indicating the environment is active).

   * **Important:** Perform **all** subsequent installation steps (SAM2 library, requirements.txt) and script executions (`python main.py ...`) **only** when the virtual environment is active.

   * **Deactivate:** When finished working on the project, you can deactivate the environment by simply running:

     ```bash
     deactivate
     ```

4. **PyTorch:** Install PyTorch according to your system (CPU/GPU, CUDA version). Follow the instructions on the [official PyTorch website](https://pytorch.org/). A GPU is **strongly recommended** for reasonable performance. Install this **after activating your virtual environment**.

5. **SAM2 Library (Manual Installation Required):** With your virtual environment **active**, install the official `sam2` library manually from its repository:

   ```bash
   # 1. Clone the repository
   git clone https://github.com/facebookresearch/sam2.git
   
   # 2. Navigate into the directory
   cd sam2
   
   # 3. Install in editable mode
   pip install -e .
   
   # 4. Go back to your project directory
   cd ..
   ```

6. **Other Python Dependencies:** With your virtual environment **active**, install the remaining required packages using the `requirements.txt` file:

   ```bash
   pip install -r requirements.txt
   ```

7. **Hugging Face Authentication (Optional but Recommended):** If you use private models or encounter download rate limits, you may need to authenticate with Hugging Face Hub **within your activated environment**:
   * **Option 1 (CLI Login):** Run `huggingface-cli login` and follow the prompts.
   * **Option 2 (Environment Variable):** Set the `HUGGING_FACE_HUB_TOKEN` environment variable with your token.

## Project Structure

```text
SAM2_analysis/
├── config/                  # Directory for configuration files
│   └── sam2_eval_config.json # Example config for the SAM2 evaluation
├── data/                    # Directory for input data (create if needed)
│   ├── images/              # Directory containing image files
│   └── degradation_map.json # Example JSON mapping image IDs to GTs and versions
├── output/                  # Directory for results (created automatically)
│   └── sam2_evaluation_results.csv # Example output file
├── sam2/                    # Cloned SAM2 library directory (from prerequisites)
├── venv/                    # Virtual environment directory (if created)
├── main.py                  # Main script to run pipelines
├── sam2_eval_pipeline.py    # Implements the SAM2 evaluation logic
├── pipeline_utils.py        # Utility functions (model loading, mask decoding)
├── metrics.py               # Functions for calculating evaluation metrics (IoU, BF1)
├── requirements.txt         # Python package dependencies
├── README.md                # This file
└── .gitignore               # Git ignore file
```

## Configuration

Experiments are defined using JSON configuration files placed in the `config/` directory.
The primary script (`main.py`) takes a single argument (`--config`) pointing to the desired configuration file.

**Example (`config/sam2_eval_config.json`):**

```json
{
  "pipeline_name": "sam2_eval", 
  "model_hf_id": "facebook/sam2-hiera-large", 
  "data_path": "data/degradation_map.json", 
  "image_base_dir": "data/images/",
  "output_path": "output/sam2_evaluation_results.csv",
  "bf1_tolerance": 2,
  "generator_config": {
    "points_per_side": 32,
    "pred_iou_thresh": 0.88,
    "stability_score_thresh": 0.95,
    "min_mask_region_area": 100 
  }
}
```

**Key Configuration Parameters:**

* `pipeline_name` (Required): Specifies which pipeline function in `main.py`'s `PIPELINE_MAP` to execute (e.g., `"sam2_eval"`).
* `model_hf_id`: The Hugging Face model identifier for the SAM2 model to load.
* `data_path`: Path (relative to root) to the JSON file mapping image IDs to ground truth RLE masks and image versions.
* `image_base_dir`: Path (relative to root) to the directory containing the actual image files referenced in `data_path`.
* `output_path`: Path (relative to root) where the resulting CSV evaluation metrics will be saved.
* `bf1_tolerance`: The tolerance in pixels used for the Boundary F1 score calculation.
* `generator_config`: A dictionary containing parameters to configure the `SamAutomaticMaskGenerator` (see `sam2` library documentation for options like `points_per_side`, `pred_iou_thresh`, etc.).

### Example `degradation_map.json`

Below is a **minimal** example of the *data map* expected by `sam2_eval_pipeline`.
Each top-level key is an **`image_id`** (any unique string).  For that image we
provide a single **COCO RLE** ground-truth mask (`ground_truth_rle`) and one or
more *versions* (original, degraded, etc.).  Paths inside `versions.*.filepath`
are **relative to** the `image_base_dir` set in the config.

```jsonc
{
  "img_001": {
    "ground_truth_rle": {
      "size": [480, 640],           // [height, width] of the mask / image
      "counts": "eNrtWEkOwjAM..."   // regular COCO RLE string
    },
    "versions": {
      "orig": {
        "filepath": "images/img_001.jpg", // relative to image_base_dir
        "level": 0                         // arbitrary numeric label
      },
      "jpeg_30": {
        "filepath": "images/img_001_jpeg30.jpg",
        "level": 30                        // e.g. JPEG quality factor
      }
    }
  },
  "img_002": {
    "ground_truth_rle": { "size": [512, 512], "counts": "c6K..." },
    "versions": {
      "orig": { "filepath": "images/img_002.jpg", "level": 0 }
    }
  }
}
```

Key rules:

* `size` **must** match the height/width of every image version.
* Add as many `versions` per image as you like; the pipeline processes each
  independently but re-uses the same GT mask.
* Extra fields (e.g. `bbox`, metadata) are ignored, so you can extend the schema
  if needed.

## Running the Evaluation

1. **Ensure Virtual Environment is Active:** Before running, activate your environment (`source venv/bin/activate` or equivalent).
2. **Prepare Data:**
   * Ensure your images are located in the directory specified by `image_base_dir` in your config file.
   * Ensure your JSON data map (like `data/degradation_map.json`) correctly references your images and contains the ground truth RLE masks. Paths within the JSON map should be relative to `image_base_dir`.
3. **Configure:** Create or modify a `.json` file in the `config/` directory with your desired settings.
4. **Execute:** Run the main script from the project root directory **while the environment is active**, providing the path to your configuration file:

   ```bash
   python main.py --config config/sam2_eval_config.json
   ```

   (Replace `config/sam2_eval_config.json` with the path to *your* chosen config file).

## Output

The script will generate a CSV file at the location specified by `output_path` in your configuration. This file contains detailed results for each image version evaluated, including:

* `image_id`: Identifier for the base image.
* `version_key`: Identifier for the specific image version (e.g., "original", "blur_level_1").
* `level`: Numerical level associated with the version.
* `relative_filepath`: Path to the image file used for this version.
* `iou`: Calculated IoU between the best predicted mask and the ground truth.
* `bf1`: Calculated Boundary F1 score for the best pair.
* `sam2_score`: The `predicted_iou` score assigned by SAM2 to the chosen best mask.
* `status`: Indicates the outcome (e.g., "Success", "Image File Not Found", "No Valid Match").

## Self-Tests and Unit Tests

This repository ships with **light-weight self-tests** embedded directly in a few
modules plus a traditional **pytest** suite.  These are meant to give immediate
feedback that all components are wired correctly after an install or a code
change.

| What to run                               | What it covers                                   |
|-------------------------------------------|--------------------------------------------------|
| `python metrics.py`                       | Numerical correctness of *mIoU* and *BF1*        |
| `python pipeline_utils.py`                | COCO-RLE decode round-trip, JSON data-map loader |
| `python sam2_eval_pipeline.py`            | End-to-end pipeline smoke test (model stubbed)   |
| `pytest` (from repo root)                 | Full unit-test suite in `tests/`                 |

The **self-tests** execute in <2 s each and require **no model download**
(they monkey-patch heavy functions).  Run them whenever you tweak core logic or
before opening a Pull Request.

Example:

```bash
# inside your activated venv
python metrics.py
python pipeline_utils.py
python sam2_eval_pipeline.py
pytest       # optional, slower
```

A CI pipeline can run the same commands to guard against regressions.

## Extending the Framework

To add a new evaluation pipeline:

1. Create a new Python script (e.g., `my_new_pipeline.py`) containing the main pipeline logic in a function (e.g., `run_my_pipeline(config)`).
2. Import your new function in `main.py`.
3. Add an entry to the `PIPELINE_MAP` dictionary in `main.py`, mapping a unique string key (e.g., `"my_new_eval"`) to your function (`run_my_pipeline`).
4. Create a new configuration file in `config/` specifying `"pipeline_name": "my_new_eval"` and any parameters your new pipeline requires.
5. Run using `python main.py --config config/my_new_config.json`.
