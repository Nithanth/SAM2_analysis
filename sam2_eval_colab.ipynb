{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# SAM2 Evaluation Pipeline - Colab Notebook\n",
    "\n",
    "This notebook provides an interactive way to run the SAM2 evaluation pipeline\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Clone repository, install dependencies, and install the required SAM2 library.\n",
    "2.  **Configuration:** Set parameters for the pipeline (model, data paths, etc.).\n",
    "3.  **Data Preparation:** Generate the `degradation_map.json` (assumes image data exists).\n",
    "4.  **(Optional) Visualization:** Inspect sample images and masks.\n",
    "5.  **Run Pipeline:** Execute the evaluation using the configured settings.\n",
    "6.  **View Results:** Load and display the output CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Base directory for the project\n",
    "# If in Colab, clone the repo. Otherwise, assume we are running from the repo root.\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Running in Colab, cloning repository...')\n",
    "    # TODO: Replace with your repo URL (use token if private)\n",
    "    # Example: !git clone https://<your_token>@github.com/YOUR_USERNAME/SAM2_analysis.git\n",
    "    %git clone https://github.com/YOUR_USERNAME/SAM2_analysis.git # <-- REPLACE THIS\n",
    "    %cd SAM2_analysis\n",
    "    PROJECT_ROOT = '/content/SAM2_analysis'\n",
    "else:\n",
    "    print('Running locally, assuming current directory is project root.')\n",
    "    # Find the project root assuming this notebook is in the root\n",
    "    PROJECT_ROOT = os.path.abspath('.')\n",
    "    # Verify by checking for a known file/directory\n",
    "    if not os.path.exists(os.path.join(PROJECT_ROOT, 'main.py')):\n",
    "        print(f'Warning: Could not confirm project root at {PROJECT_ROOT}')\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "os.chdir(PROJECT_ROOT) # Ensure we are in the project root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "print('\\nInstalling dependencies...')\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the SAM2 library\n",
    "# Assumes the sam2 code is located in 'external/sam2' within the project\n",
    "print('\\nInstalling SAM2 library...')\n",
    "SAM2_DIR = os.path.join(PROJECT_ROOT, 'external/sam2')\n",
    "\n",
    "if not os.path.exists(SAM2_DIR):\n",
    "    print(f'Error: SAM2 directory not found at {SAM2_DIR}')\n",
    "    print('Please ensure you have cloned the SAM2 repository into external/sam2')\n",
    "    # Optional: Add command to clone it if missing\n",
    "    # print('Attempting to clone SAM2...')\n",
    "    # !git clone <SAM2_REPO_URL> external/sam2 # <-- Add SAM2 repo URL if desired\n",
    "else:\n",
    "    # Use pip install -e for editable install\n",
    "    %pip install -e \"{SAM2_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline Configuration ---\n",
    "# Mimic the structure of sam2_eval_config.json\n",
    "\n",
    "config = {\n",
    "    \"pipeline_name\": \"sam2_eval\",\n",
    "    \"description\": \"Evaluate SAM2 auto-mask generator on data map (Colab)\",\n",
    "\n",
    "    # --- Data Configuration ---\n",
    "    # Path to the generated data map (relative to project root)\n",
    "    \"data_path\": \"data/degradation_map.json\",\n",
    "\n",
    "    # Base directory where image files referenced in data_path are located\n",
    "    # Paths in data_path['versions'][*]['filepath'] are relative to this.\n",
    "    \"image_base_dir\": \"data\", # Assumes images are in data/images, data/pic_degraded/*\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    # Hugging Face identifier for the SAM2 model\n",
    "    # Examples: 'facebook/sam2-hiera-tiny', 'facebook/sam2-hiera-small',\n",
    "    #           'facebook/sam2-hiera-base', 'facebook/sam2-hiera-large'\n",
    "    \"model_hf_id\": \"facebook/sam2-hiera-tiny\", # Use a smaller model for faster testing\n",
    "\n",
    "    # --- Mask Generator Configuration ---\n",
    "    # Parameters passed to SAM2AutomaticMaskGenerator\n",
    "    # See SAM2 library documentation for all options\n",
    "    \"generator_config\": {\n",
    "        \"points_per_side\": 16,       # Lower for faster processing\n",
    "        \"pred_iou_thresh\": 0.80,     # Default: 0.88\n",
    "        \"stability_score_thresh\": 0.90, # Default: 0.95\n",
    "        \"crop_n_layers\": 0,          # Default: 0 (no cropping)\n",
    "        \"min_mask_region_area\": 10   # Default: 0\n",
    "    },\n",
    "\n",
    "    # --- Evaluation Metric Configuration ---\n",
    "    \"iou_threshold\": 0.5,        # For matching pred mask to GT mask\n",
    "    \"bf1_tolerance\": 2,          # Tolerance in pixels for Boundary F1 score\n",
    "\n",
    "    # --- Output Configuration ---\n",
    "    # Directory to save the results CSV file (relative to project root)\n",
    "    \"output_dir\": \"output\",\n",
    "    \"results_filename_prefix\": \"results_colab_\"\n",
    "}\n",
    "\n",
    "# Make directories absolute for clarity later\n",
    "config['data_path'] = os.path.join(PROJECT_ROOT, config['data_path'])\n",
    "config['image_base_dir'] = os.path.join(PROJECT_ROOT, config['image_base_dir'])\n",
    "config['output_dir'] = os.path.join(PROJECT_ROOT, config['output_dir'])\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "import json\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "**IMPORTANT:** This section assumes the necessary image files (e.g., in `data/images/`, `data/pic_degraded/*`) and any COCO annotation files needed by `code_json.py` are already present in the `data/` directory of your Colab environment/mounted drive.\n",
    "\n",
    "If they are not present, you need to:\n",
    "1. Run `data/data_scripts/code_degradation.py` locally first (if you haven't).\n",
    "2. Upload the entire `data/` directory to Colab or sync via Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Ensure os is imported if running cells independently\n",
    "\n",
    "# Ensure data and output directories exist\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "OUTPUT_DIR = config['output_dir'] # Use absolute path from config\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f'Checking for image base directory: {config[\"image_base_dir\"]}')\n",
    "print(f'Expected output map path: {config[\"data_path\"]}')\n",
    "\n",
    "# Check for a key data file (e.g., the script expects COCO annotations)\n",
    "expected_coco_file = os.path.join(DATA_DIR, 'annotations', 'instances_val2017_100.json') # Example path\n",
    "if not os.path.exists(expected_coco_file):\n",
    "    print(f\"Warning: Expected COCO annotation file not found at {expected_coco_file}. \"\n",
    "          f\"The 'code_json.py' script might fail if it relies on this.\")\n",
    "# Add checks for other necessary data/image directories if needed\n",
    "\n",
    "# Run the script to generate the degradation_map.json\n",
    "print('\\nRunning script to generate degradation_map.json...')\n",
    "# Note: Ensure code_json.py uses relative paths correctly or adjust it if needed\n",
    "%python data/data_scripts/code_json.py\n",
    "\n",
    "# Verify the map was created\n",
    "data_map_path = config['data_path'] # Use absolute path from config\n",
    "if os.path.exists(data_map_path):\n",
    "    print(f'Successfully generated {data_map_path}')\n",
    "else:\n",
    "    print(f'Error: {data_map_path} was not generated. Check data availability and script output.')\n",
    "    # Add more detailed error checking if the script provides specific logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4. (Optional) Visualize Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util # Import for RLE decoding\n",
    "import os # Ensure os is imported\n",
    "\n",
    "def visualize_sample(data_map_path, image_base_dir):\n",
    "    \"\"\"Loads the data map, picks a random image, and displays its versions and GT mask.\"\"\"\n",
    "    if not os.path.exists(data_map_path):\n",
    "        print(f'Cannot visualize: {data_map_path} not found.')\n",
    "        return\n",
    "\n",
    "    with open(data_map_path, 'r') as f:\n",
    "        try:\n",
    "            data_map = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error reading data map JSON: {e}\")\n",
    "            return\n",
    "\n",
    "\n",
    "    if not data_map:\n",
    "        print('Cannot visualize: Data map is empty.')\n",
    "        return\n",
    "\n",
    "    image_id = random.choice(list(data_map.keys()))\n",
    "    print(f'Visualizing sample for image_id: {image_id}')\n",
    "    item_data = data_map[image_id]\n",
    "\n",
    "    # Decode GT mask\n",
    "    gt_rle = item_data.get('ground_truth_rle')\n",
    "    gt_mask = None\n",
    "    if gt_rle:\n",
    "        try:\n",
    "            # Handle potential string vs dict RLE formats if needed\n",
    "            if isinstance(gt_rle, str): # If RLE is just the counts string\n",
    "                 # Need size info - assume it's stored elsewhere or reconstruct\n",
    "                 print(\"Warning: GT RLE is string, size info needed for decoding.\")\n",
    "                 # Example: Need to fetch item_data['height'], item_data['width']\n",
    "                 # gt_rle_dict = {'size': [item_data['height'], item_data['width']], 'counts': gt_rle}\n",
    "                 # gt_mask = mask_util.decode(gt_rle_dict)\n",
    "            elif isinstance(gt_rle, dict):\n",
    "                 gt_mask = mask_util.decode(gt_rle)\n",
    "            else:\n",
    "                 print(f\"Warning: Unexpected GT RLE format: {type(gt_rle)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'  Could not decode GT RLE: {e}')\n",
    "\n",
    "\n",
    "    # Count versions - needs robust handling of structure\n",
    "    num_versions = 0\n",
    "    versions_to_plot = []\n",
    "    base_img_path = image_base_dir # Already absolute\n",
    "\n",
    "    if 'versions' in item_data:\n",
    "         for degradation_type, levels_or_data in item_data['versions'].items():\n",
    "              if isinstance(levels_or_data, dict) and 'filepath' in levels_or_data: # e.g., 'original'\n",
    "                   filepath = levels_or_data['filepath']\n",
    "                   level = levels_or_data.get('level', 'N/A')\n",
    "                   title = f'{degradation_type}\\n(Level: {level})'\n",
    "                   # Construct absolute path carefully based on structure\n",
    "                   abs_path = os.path.join(base_img_path, filepath) # Assumes filepath is relative to base_img_dir\n",
    "                   versions_to_plot.append({'title': title, 'path': abs_path})\n",
    "                   num_versions += 1\n",
    "              elif isinstance(levels_or_data, dict): # Nested levels like {'1': {...}, '2': {...}}\n",
    "                   for level, version_data in levels_or_data.items():\n",
    "                       if isinstance(version_data, dict) and 'filepath' in version_data:\n",
    "                           filepath = version_data['filepath']\n",
    "                           level_val = version_data.get('level', level) # Use nested level if available\n",
    "                           title = f'{degradation_type}_{level}\\n(Level: {level_val})'\n",
    "                           # Construct absolute path\n",
    "                           abs_path = os.path.join(base_img_path, filepath) # Assumes filepath relative to base\n",
    "                           versions_to_plot.append({'title': title, 'path': abs_path})\n",
    "                           num_versions += 1\n",
    "\n",
    "    plot_cols = num_versions + (1 if gt_mask is not None else 0)\n",
    "    if plot_cols == 0:\n",
    "        print(\"No image versions or GT mask found to plot.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, max(1, plot_cols), figsize=(5 * max(1, plot_cols), 5))\n",
    "    if plot_cols == 1:\n",
    "        axes = [axes] # Make it iterable\n",
    "\n",
    "    plot_idx = 0\n",
    "\n",
    "    # Display versions\n",
    "    for version_info in versions_to_plot:\n",
    "        img_path = version_info['path']\n",
    "        title = version_info['title']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            axes[plot_idx].imshow(img)\n",
    "            axes[plot_idx].set_title(title)\n",
    "        except FileNotFoundError:\n",
    "            print(f'  Image not found: {img_path}')\n",
    "            axes[plot_idx].set_title(f'{title}\\n(Not Found)')\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading image {img_path}: {e}\")\n",
    "             axes[plot_idx].set_title(f'{title}\\n(Load Error)')\n",
    "        finally:\n",
    "            axes[plot_idx].axis('off')\n",
    "            plot_idx += 1\n",
    "\n",
    "\n",
    "    # Display GT mask\n",
    "    if gt_mask is not None:\n",
    "        if plot_idx < len(axes): # Ensure we don't go out of bounds\n",
    "            axes[plot_idx].imshow(gt_mask, cmap='gray')\n",
    "            axes[plot_idx].set_title('Ground Truth Mask')\n",
    "            axes[plot_idx].axis('off')\n",
    "        else:\n",
    "             print(\"Warning: Not enough subplot axes allocated for GT mask.\")\n",
    "\n",
    "    # Hide unused axes\n",
    "    for i in range(plot_idx + (1 if gt_mask is not None else 0), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Run visualization ---\n",
    "data_map_path = config['data_path']\n",
    "image_base_dir = config['image_base_dir']\n",
    "if os.path.exists(data_map_path):\n",
    "    visualize_sample(data_map_path, image_base_dir)\n",
    "else:\n",
    "    print(f\"Skipping visualization because data map not found: {data_map_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os # Ensure os is imported\n",
    "\n",
    "# Ensure project root is in path for imports\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Import the main pipeline function\n",
    "try:\n",
    "    from sam2_eval_pipeline import run_evaluation_pipeline\n",
    "    print('Imported run_evaluation_pipeline successfully.')\n",
    "except ImportError as e:\n",
    "    print(f'Error importing pipeline function: {e}')\n",
    "    print('Ensure installation steps completed correctly and you are in the project root.')\n",
    "    run_evaluation_pipeline = None # Prevent further errors\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during import: {e}\")\n",
    "     run_evaluation_pipeline = None\n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "results_df = None\n",
    "if run_evaluation_pipeline:\n",
    "    print('\\nStarting evaluation pipeline...')\n",
    "    try:\n",
    "        # Use the absolute paths from the config dictionary defined earlier\n",
    "        results_df = run_evaluation_pipeline(\n",
    "            data_path=config['data_path'],\n",
    "            image_base_dir=config['image_base_dir'],\n",
    "            model_hf_id=config['model_hf_id'],\n",
    "            generator_config=config['generator_config'],\n",
    "            iou_threshold=config['iou_threshold'],\n",
    "            bf1_tolerance=config['bf1_tolerance'],\n",
    "            output_dir=config['output_dir'],\n",
    "            results_filename_prefix=config['results_filename_prefix']\n",
    "        )\n",
    "        if results_df is not None:\n",
    "            print(f'Pipeline finished successfully. Results saved in {config[\"output_dir\"]}')\n",
    "        else:\n",
    "            # The function might return None on failure/no data\n",
    "             print('Pipeline function completed, but returned None (possibly no data processed or an error occurred). Check logs.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during pipeline execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "else:\n",
    "     print(\"Skipping pipeline execution due to import failure.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 6. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os # Ensure os is imported\n",
    "\n",
    "output_path = config['output_dir'] # Use absolute path\n",
    "prefix = config['results_filename_prefix']\n",
    "\n",
    "if results_df is not None and not results_df.empty:\n",
    "    print('Displaying results DataFrame returned from pipeline:')\n",
    "    # Configure pandas display options if needed\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    # pd.set_option('display.max_columns', None)\n",
    "    try:\n",
    "        from google.colab.data_table import DataTable # Use Colab's interactive table\n",
    "        display(DataTable(results_df))\n",
    "    except ImportError:\n",
    "        display(results_df) # Fallback for non-Colab\n",
    "\n",
    "elif os.path.exists(output_path):\n",
    "    # Try to find the latest results CSV in the output directory if DF is empty/None\n",
    "    print('Results DataFrame not available directly. Trying to load latest CSV from output directory...')\n",
    "    try:\n",
    "        csv_files = [f for f in os.listdir(output_path) if f.startswith(prefix) and f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            # Find the most recently modified CSV\n",
    "            latest_csv = max(csv_files, key=lambda f: os.path.getmtime(os.path.join(output_path, f)))\n",
    "            latest_csv_path = os.path.join(output_path, latest_csv)\n",
    "            print(f'Loading latest results file: {latest_csv_path}')\n",
    "            results_df_loaded = pd.read_csv(latest_csv_path)\n",
    "            print('Displaying loaded results DataFrame:')\n",
    "            try:\n",
    "                from google.colab.data_table import DataTable\n",
    "                display(DataTable(results_df_loaded))\n",
    "            except ImportError:\n",
    "                 display(results_df_loaded)\n",
    "        else:\n",
    "            print(f'No results CSV files found in {output_path} matching prefix \"{prefix}\"')\n",
    "    except FileNotFoundError:\n",
    "         print(f'Output directory {output_path} seems to have disappeared.')\n",
    "    except Exception as e:\n",
    "         print(f'Error loading or listing results CSV: {e}')\n",
    "else:\n",
    "     print(f\"Neither results DataFrame nor output directory ({output_path}) found.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
